# NeuroAI-Readings (Topical Organization)

I've been incrementally updating this list since Jan 29, 2024 and I find this a good way of collecting papers from random sources (conferences, twitter, conversations, etc.). 

<img width="1126" alt="word_cloud" src="https://github.com/user-attachments/assets/d3c3690a-ad98-435d-ae03-96e44748bc98" />

This version selects some entries by theme/topic for easier reference. For the more frequently updated chronological version, see [README-chronological.md](https://github.com/Daniel-Gong/NeuroAI-Readings/edit/main/READMEchronological.md).

---

## Table of Contents

- [Neural Representation, Geometry, and Manifolds](#neural-representation-geometry-and-manifolds)
- [Memory: Working, Episodic, and Associative](#memory-working-episodic-and-associative)
- [Transformers, Attention, and Large Language Models](#transformers-attention-and-large-language-models)
- [Predictive Coding, Energy Efficiency, and Neural Computation](#predictive-coding-energy-efficiency-and-neural-computation)
- [Causality, Reasoning, and Cognitive Science](#causality-reasoning-and-cognitive-science)
- [Reinforcement Learning, Planning, and Control](#reinforcement-learning-planning-and-control)
- [Robotics, Embodiment, and Spiking Networks](#robotics-embodiment-and-spiking-networks)
- [Neuroscience Reviews, Critiques, and Meta](#neuroscience-reviews-critiques-and-meta)
- [Mathematics, Statistics, and Methodology](#mathematics-statistics-and-methodology)
- [Interpretability, Mechanistic and Explanatory](#interpretability-mechanistic-and-explanatory)
- [Benchmark Datasets, Cognitive Tests, and Evaluation](#benchmark-datasets-cognitive-tests-and-evaluation)
- [Books, Textbooks, and Lecture Notes](#books-textbooks-and-lecture-notes)
- [Miscellaneous, Philosophy, and Other](#miscellaneous-philosophy-and-other)

---

## Neural Representation, Geometry, and Manifolds

- [Neural tuning and representational geometry](https://www.nature.com/articles/s41583-021-00502-3)
- [Three aspects of representation in neuroscience](https://www.sciencedirect.com/science/article/pii/S1364661322002108)
- [Distributed representations of words and phrases and their compositionality](https://papers.nips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
- [Shared Representational Geometry Across Neural Networks](https://arxiv.org/abs/1811.11684)
- [The Geometry of Concepts: Sparse Autoencoder Feature Structure](https://arxiv.org/abs/2410.19750)
- [Tracking the topology of neural manifolds across populations](https://www.pnas.org/doi/10.1073/pnas.2407997121)
- [Manifolds: A Gentle Introduction](https://bjlkeng.io/posts/manifolds/)
- [Dimension Reduction using Isomap](https://medium.com/data-science-in-your-pocket/dimension-reduction-using-isomap-72ead0411de)
- [A neural manifold view of the brain](https://www.nature.com/articles/s41593-025-02031-z)

## Memory: Working, Episodic, and Associative

- [A formal model of capacity limits in working memory](https://www.sciencedirect.com/science/article/pii/S0749596X06000982)
- [Representation and computation in visual working memory](https://www.nature.com/articles/s41562-024-01871-2?fromPaywallRec=false)
- [The capacity of visual working memory for features and conjunctions](https://www.nature.com/articles/36846)
- [The Distributed Nature of Working Memory](https://www.sciencedirect.com/science/article/pii/S1364661316302170)
- [Theories of Error Back-Propagation in the Brain](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30012-9)
- [On prefrontal working memory and hippocampal episodic memory: Unifying memories stored in weights and activity slots](https://www.biorxiv.org/content/10.1101/2023.11.05.565662v2.full)
- [Adaptive chunking improves effective working memory capacity in a prefrontal cortex and basal ganglia circuit](https://www.biorxiv.org/content/10.1101/2024.03.24.586455v1.full)
- [Attractor dynamics with activity-dependent plasticity capture human working memory across time scales](https://www.nature.com/articles/s44271-023-00027-8)
- [A generative model of memory construction and consolidation](https://www.nature.com/articles/s41562-023-01799-z)
- [Abstract representations emerge in human hippocampal neurons during inference](https://www.nature.com/articles/s41586-024-07799-x)
- [Memory in humans and deep language models: Linking hypotheses for model augmentation](https://arxiv.org/abs2210.1869)
- [Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/pdf/2304.15004.pdf)
- [Mechanism for feature learning in neural networks and backpropagation-free machine learning models](https://www.science.org/doi/10.1126/science.adi5639)

## Transformers, Attention, and Large Language Models

- [Transformer as a hippocampal memory consolidation model based on NMDAR-inspired nonlinearity](https://openreview.net/pdf?id=vKpVJxplmB)
- [Attention is not all you need anymore](https://arxiv.org/pdf/2308.07661.pdf)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Attention and Memory in Deep Learning](https://www.youtube.com/watch?v=AIiwuClvH6k)
- [Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks](https://arxiv.org/abs/2402.08211)
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
- [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)
- [Representational Strengths and Limitations of Transformers](https://openreview.net/forum?id=36DxONZ9bA)
- [On the Emergence of Position Bias in Transformers](https://arxiv.org/abs/2502.01951)
- [Transformers: a Primer](https://www.columbia.edu/~jsl2239/transformers.html)
- [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)
- [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)
- [Transformers, parallel computation, and logarithmic depth](https://arxiv.org/pdf/2402.09268)
- [Mastering Decoder-Only Transformer: A Comprehensive Guide](https://www.analyticsvidhya.com/blog/2024/04/mastering-decoder-only-transformer-a-comprehensive-guide/#:~:text=Typically%2C%20scaling%20up%2...)
- [Transformers as Support Vector Machines](https://arxiv.org/abs/2308.16898)
- [Do Language Models Have a Critical Period for Language Acquisition?](https://arxiv.org/abs/2407.19325#)
- [RNNs Implicitly Implement Tensor Product Representations](https://arxiv.org/abs/1812.08718)
- [Tensor product variable binding and the representation of symbolic structures in connectionist systems](https://www.sciencedirect.com/science/article/pii/000437029090007M#:~:text=The%20representation...)
- [Memory Networks: Towards Fully Biologically Plausible Learning](https://arxiv.org/abs/2409.17282)
- [Upper and lower memory capacity bounds of transformers for next-token prediction](https://arxiv.org/abs/2405.13718)
- [TransformerFAM: Feedback attention is working memory](https://arxiv.org/abs/2404.09173)
- [STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making](https://arxiv.org/abs/2405.16376)
- [Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://arxiv.org/abs/2402.12875)
- [Theoretical Limitations of Self-Attention in Neural Sequence Models](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00306/43545/Theoretical-Limitations-of-Self-Attention-in)
- [Self-attention Does Not Need $O(n^2)$ Memory](https://arxiv.org/abs/2112.05682)
- [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)
- [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
- [LLMs are Bayesian, in Expectation, not in Realization](https://arxiv.org/abs/2507.11768)
- [Lost in Embeddings: Information Loss in Vision-Language Models](https://arxiv.org/abs/2509.11986)

## Predictive Coding, Energy Efficiency, and Neural Computation

- [Predictive Coding: a Theoretical and Experimental Review](https://arxiv.org/abs/2107.12979)
- [Predictive coding is a consequence of energy efficiency in recurrent neural networks](https://www.sciencedirect.com/science/article/pii/S2666389922002719?via%3Dihub)
- [On Neural Differential Equations](https://arxiv.org/abs/2202.02435)
- [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366)
- [Continuous Attractor Neural Networks: Candidate of a Canonical Model for Neural Information Representation](https://www.semanticscholar.org/reader/9bd6f45364067c8ebf296fc374087f8f7f797fa4)
- [Hopfield Networks is All You Need](https://arxiv.org/abs/2008.02217)
- [Learning Attractor Dynamics for Generative Memory](https://proceedings.neurips.cc/paper_files/paper/2018/file/6e4243f5511fd6ef0f03e9f386d54403-Paper.pdf)
- [Statistical mechanics of complex neural systems and high dimensional data](https://arxiv.org/abs/1301.7115)
- [Wake-sleep transition as a noisy bifurcation](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.94.022412)

## Causality, Reasoning, and Cognitive Science

- [On the Paradox of Learning to Reason from Data](https://arxiv.org/pdf/2205.11502.pdf)
- [CRAB: Assessing the Strength of Causal Relationships Between Real-World Events](https://arxiv.org/pdf/2311.04284.pdf)
- [Passive learning of active causal strategies in agents and language models](https://arxiv.org/pdf/2305.16183.pdf)
- [SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning](https://arxiv.org/pdf/2104.05832.pdf)
- [A Critical Review of Causal Reasoning Benchmarks for Large Language Models](https://openreview.net/pdf?id=mRwgczYZFJ)
- [Predictive Coding: a Theoretical and Experimental Review](https://arxiv.org/abs/2107.12979)
- [Cognitive computational neuroscience](https://www.nature.com/articles/s41593-018-0210-5)
- [Testing theory of mind in large language models and humans](https://www.nature.com/articles/s41562-024-01882-z)
- [RAVEN: A Dataset for Relational and Analogical Visual rEasoNing](https://arxiv.org/abs/1903.02741)
- [Compositionality Decomposed: How do Neural Networks Generalise?](https://jair.org/index.php/jair/article/view/11674/26576)
- [Compositional architecture: Orthogonal neural codes for task context and spatial memory in prefrontal cortex](https://www.biorxiv.org/content/10.1101/2025.02.25.640211v1)
- [Reasoning ability is (little more than) working-memory capacity?!](https://www.sciencedirect.com/science/article/pii/S0160289605800121)

## Reinforcement Learning, Planning, and Control

- [Distributional Reinforcement Learning in the Brain](https://www.sciencedirect.com/science/article/pii/S0166223620301983?via%3Dihub)
- [Reinforcement Learning: An Overview](https://arxiv.org/abs/2412.05265)
- [Reinforcement Learning: An Introduction](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
- [Deep reinforcement learning can promote sustainable human behaviour in a common-pool resource problem](https://www.nature.com/articles/s41467-025-58043-7)
- [Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems](https://arxiv.org/pdf/1512.08756)
- [Prefrontal cortex as a meta-reinforcement learning system](https://www.nature.com/articles/s41593-018-0147-8)

## Robotics, Embodiment, and Spiking Networks

- [Bridging Neuroscience and Robotics: Spiking Neural Networks in Action](https://www.mdpi.com/1424-8220/23/21/8880)
- [Combined Sensing, Cognition, Learning, and Control for Developing Future Neuro-Robotics Systems: A Survey](https://ieeexplore.ieee.org/document/8634917)
- [AI, Robotics & Neuroengineering at Ken Kennedy Institute](https://kenkennedy.rice.edu/research/neuroengineering-ai-robotics)
- [Special Issue : Applications of Neural Networks in Robot Control](https://www.mdpi.com/journal/robotics/special_issues/194M9AW732)
- [Embodied AI Workshop](https://embodied-ai.org/)
- [A call for embodied AI](https://arxiv.org/abs/2402.03824)
- [Spiking neural networks: Towards bio-inspired multimodal perception in robotics](https://arxiv.org/abs/2411.14147)
- [PsychRNN: An Accessible and Flexible Python Package for Training Recurrent Neural Network Models on Cognitive Tasks](https://www.eneuro.org/content/8/1/ENEURO.0427-20.2020)

## Neuroscience Reviews, Critiques, and Meta

- [Neuroscience needs behavior](https://www.cell.com/neuron/pdf/S0896-6273%2816%2931040-6.pdf)
- [No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit](https://openreview.net/pdf?id=mxi1xKzNFrb)
- [NeuroAI critique: What have we learned about artificial intelligence from studying the brain?](https://link.springer.com/article/10.1007/s00422-024-00983-2#:~:text=It%20has%20been%20argued%20that,mos...)
- [A Review of Neuroscience-Inspired Machine Learning](https://arxiv.org/pdf/2403.18929.pdf)
- [The new NeuroAI](https://www.nature.com/articles/s42256-024-00826-6)
- [Trainees’ perspectives and recommendations for catalyzing the next generation of NeuroAI researchers](https://www.nature.com/articles/s41467-024-53375-2)
- [Future views on neuroscience and AI](https://www.cell.com/cell/fulltext/S0092-8674\(24\)01089-4)
- [Averaging is a convenient fiction of neuroscience](https://www.thetransmitter.org/neural-coding/averaging-is-a-convenient-fiction-of-neuroscience/)
- [Circular and unified analysis in network neuroscience](https://elifesciences.org/articles/79559)
- [Circular analysis in systems neuroscience: the dangers of double dipping](https://www.nature.com/articles/nn.2303)
- [From circuits to behavior: a bridge too far?](https://www.nature.com/articles/nn.3043)

## Mathematics, Statistics, and Methodology

- [The Matrix Calculus You Need For Deep Learning](https://arxiv.org/abs/1802.01528)
- [Notes on Quadratic Forms](https://rmi.tsu.ge/~kade/LecturesT.Kadeishvili/MathEconomics/Term3/Week3QuadraticLEC.pdf)
- [Vector Calculus Notes](https://www.damtp.cam.ac.uk/user/tong/vc/vc.pdf)
- [Modern Quantum Mechanics.pdf](https://www.fisica.net/mecanica-quantica/Sakurai%20-%20Modern%20Quantum%20Mechanics.pdf)
- [Book Of Proof](https://www.people.vcu.edu/~rhammack/BookOfProof/Main.pdf)
- [Everything You Wanted To Know About Mathematics](https://www.math.cmu.edu/~jmackey/151_128/bws_book.pdf)
- [Handbook of Mathematics](https://link.springer.com/book/10.1007/978-3-662-46221-8#overview)
- [Probabilistic Machine Learning](https://probml.github.io/pml-book/book1.html)

## Interpretability, Mechanistic and Explanatory

- [The Urgency of Interpretability](https://www.darioamodei.com/post/the-urgency-of-interpretability)
- [Interpretability, Mechanistic and Explanatory](https://ieeexplore.ieee.org/abstract/document/10136140)
- [A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i](https://www.arxiv.org/abs/2505.00808)
- [Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii](https://arxiv.org/abs/2505.01372)
- [Mechanistic Interpretability for AI Safety -- A Review](https://arxiv.org/abs/2404.14082)
- [Layer by Layer: Uncovering Hidden Representations in Language Models](https://arxiv.org/abs/2502.02013)
- [Transformer Interpretability Beyond Attention Visualization](https://paperswithcode.com/paper/transformer-interpretability-beyond-attention)

## Benchmark Datasets, Cognitive Tests, and Evaluation

- [Testing theory of mind in large language models and humans](https://www.nature.com/articles/s41562-024-01882-z)
- [RAVEN: A Dataset for Relational and Analogical Visual rEasoNing](https://arxiv.org/abs/1903.02741)
- [SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning](https://arxiv.org/pdf/2104.05832.pdf)
- [DevBench: A multimodal developmental benchmark for language learning](https://arxiv.org/abs/2406.10215)

## Books, Textbooks, and Lecture Notes

- [The little book of deep learning](https://fleuret.org/public/lbdl.pdf)
- [The Handbook of Brain Theory and Neural Networks](https://direct.mit.edu/books/edited-volume/4358/The-Handbook-of-Brain-Theory-and-Neural-Networks)
- [Textbook: Introduction to Machine Learning](https://arxiv.org/abs/2409.02668)
- [Infinite Powers: How Calculus Reveals the Secrets of the Universe](https://books.google.com/books/about/Infinite_Powers.html?id=WJlPuQEACAAJ)
- [The Self-Assembling Brain: How Neural Networks Grow Smarter](https://books.google.com/books?id=DEgHEAAAQBAJ&printsec=copyright#v=onepage&q&f=false)
- [Handbook of Mathematics](https://link.springer.com/book/10.1007/978-3-662-46221-8#overview)
- [Probabilistic Machine Learning](https://probml.github.io/pml-book/book1.html)
- [How To Build Conscious Machines](https://osf.io/preprints/thesiscommons/wehmg_v1?view_only=)

## Miscellaneous, Philosophy, and Other

- [Why Is Anything Conscious?](https://osf.io/preprints/osf/mtgn7)
- [Is gravity evidence of a computational universe?](https://pubs.aip.org/aip/adv/article/15/4/045035/3345217/Is-gravity-evidence-of-a-computational-universe)
- [Intelligence at the Edge of Chaos](https://www.arxiv.org/abs/2410.02536)
- [A theory of consciousness from a theoretical computer science perspective: Insights from the Conscious Turing Machine](https://www.pnas.org/doi/full/10.1073/pnas.2115934119)
- [The Brain–Cognitive Behavior Problem: A Retrospective](https://www.eneuro.org/content/7/4/ENEURO.0069-20.2020)
- [Against the Epistemological Primacy of the Hardware: The Brain from Inside Out, Turned Upside Down](https://www.eneuro.org/content/7/4/ENEURO.0215-20.2020)

---

*If you have suggestions for better topic groupings, or want to contribute, feel free to open a PR!*
